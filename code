# Databricks Notebook: Real-Time Sales ETL Pipeline
# Author: Urmila Murri
# Date: Aug 2024

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, year, month, sum as _sum
from pyspark.sql.types import DoubleType, DateType

# Initialize Spark
spark = SparkSession.builder.appName("SalesETLPipeline").getOrCreate()

# 1. Ingest Raw CSV
raw_df = spark.read.option("header", "true").csv("/FileStore/tables/sales_data.csv")

# 2. Clean & Transform
cleaned_df = (raw_df
    .withColumn("Order Date", to_date(col("Order Date"), "MM/dd/yyyy"))
    .withColumn("Sales", col("Sales").cast(DoubleType()))
    .withColumn("Profit", col("Profit").cast(DoubleType()))
    .dropna(subset=["Order ID", "Sales"])
    .dropDuplicates(["Order ID"])
    .filter(col("Order Date").isNotNull())
)

# 3. Monthly Aggregations
monthly_sales = (cleaned_df
    .groupBy(year("Order Date").alias("Year"), month("Order Date").alias("Month"))
    .agg(
        _sum("Sales").alias("Total_Sales"),
        _sum("Profit").alias("Total_Profit")
    )
    .orderBy("Year", "Month")
)

# 4. Save to Delta Lake
cleaned_df.write.format("delta").mode("overwrite").save("/delta/sales_cleaned")
monthly_sales.write.format("delta").mode("overwrite").save("/delta/monthly_summary")

# 5. Register Tables
spark.sql("CREATE TABLE IF NOT EXISTS sales_cleaned USING DELTA LOCATION '/delta/sales_cleaned'")
spark.sql("CREATE TABLE IF NOT EXISTS monthly_summary USING DELTA LOCATION '/delta/monthly_summary'")

print("ETL Pipeline Completed & Delta Tables Created!")
display(monthly_sales)
